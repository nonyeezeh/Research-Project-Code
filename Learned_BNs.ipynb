{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2SsCFhts4U9dzipcn7my+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nonyeezeh/Research-Project-Code/blob/main/Learned_BNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "_Mfu2JceAwep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pgmpy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qEX38oqm7e_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "491f4976-2850-439a-fc1e-3aa628d17d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pgmpy\n",
            "  Downloading pgmpy-0.1.26-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pgmpy) (3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pgmpy) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pgmpy) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pgmpy) (1.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pgmpy) (2.1.4)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from pgmpy) (3.1.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pgmpy) (2.4.1+cu121)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from pgmpy) (0.14.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pgmpy) (4.66.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from pgmpy) (1.4.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from pgmpy) (3.3.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from pgmpy) (2.1.1)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (from pgmpy) (0.7.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai->pgmpy) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai->pgmpy) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai->pgmpy) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai->pgmpy) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai->pgmpy) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai->pgmpy) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai->pgmpy) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai->pgmpy) (1.24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->pgmpy) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pgmpy) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pgmpy) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pgmpy) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->pgmpy) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels->pgmpy) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pgmpy) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pgmpy) (1.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pgmpy) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->pgmpy) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost->pgmpy) (2.23.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai->pgmpy) (1.65.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai->pgmpy) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai->pgmpy) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai->pgmpy) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai->pgmpy) (4.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels->pgmpy) (1.16.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai->pgmpy) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai->pgmpy) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai->pgmpy) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pgmpy) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai->pgmpy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai->pgmpy) (2.23.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pgmpy) (1.3.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai->pgmpy) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai->pgmpy) (1.48.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai->pgmpy) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (2024.8.30)\n",
            "Downloading pgmpy-0.1.26-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pgmpy\n",
            "Successfully installed pgmpy-0.1.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pgmpy.models import BayesianNetwork\n",
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.factors.discrete import TabularCPD\n",
        "from pgmpy.sampling import BayesianModelSampling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from pgmpy.estimators import HillClimbSearch, BicScore, AICScore, MaximumLikelihoodEstimator\n",
        "from pgmpy.inference import VariableElimination\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import entropy\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "4QgQJ6dW1tOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "9ULJOsaaDsIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Network Data Generation 500, 1000, 1500, ..., 10000 Samples (dense)"
      ],
      "metadata": {
        "id": "g10HEJAK6skF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#np.random.seed(1)\n",
        "\n",
        "# Define the mappings for IR, EI, SP\n",
        "ir_map = {0: 'low', 1: 'medium', 2: 'high'}\n",
        "ei_map = {0: 'poor', 1: 'average', 2: 'good'}\n",
        "sp_map = {0: 'decrease', 1: 'stable', 2: 'increase'}\n",
        "\n",
        "# Define the dense Bayesian Network\n",
        "dense_model = BayesianNetwork([('IR', 'EI'), ('EI', 'SP'), ('IR', 'SP')])\n",
        "\n",
        "# Function to generate CPDs\n",
        "def generate_cpds():\n",
        "    ir_probs = np.random.rand(3)\n",
        "    ir_probs /= ir_probs.sum()  # Normalize to make it a valid probability distribution\n",
        "\n",
        "    ei_given_ir_probs = np.random.rand(3, 3)\n",
        "    ei_given_ir_probs /= ei_given_ir_probs.sum(axis=0, keepdims=True)\n",
        "\n",
        "    sp_probs = np.random.rand(3, 3, 3)\n",
        "    sp_probs /= sp_probs.sum(axis=0, keepdims=True)\n",
        "\n",
        "    sp_probs_reshaped = sp_probs.reshape(3, -1)\n",
        "\n",
        "    return ir_probs, ei_given_ir_probs, sp_probs_reshaped\n",
        "\n",
        "# Save probabilities in a single CSV file\n",
        "def save_probabilities(ir_probs, ei_probs, sp_probs, filename):\n",
        "    # Create a DataFrame for IR probabilities\n",
        "    ir_df = pd.DataFrame({\n",
        "        'IR_State': ['low', 'medium', 'high'],\n",
        "        'IR_Prob': ir_probs\n",
        "    })\n",
        "\n",
        "    # Create a DataFrame for EI given IR probabilities\n",
        "    ei_df = pd.DataFrame(ei_probs, columns=['EI_given_IR_low', 'EI_given_IR_medium', 'EI_given_IR_high'])\n",
        "    ei_df['EI_State'] = ['poor', 'average', 'good']\n",
        "\n",
        "    # Create a DataFrame for SP given IR and EI probabilities\n",
        "    sp_df = pd.DataFrame(sp_probs, columns=[\n",
        "        'SP_given_IR_low_EI_poor', 'SP_given_IR_low_EI_average', 'SP_given_IR_low_EI_good',\n",
        "        'SP_given_IR_medium_EI_poor', 'SP_given_IR_medium_EI_average', 'SP_given_IR_medium_EI_good',\n",
        "        'SP_given_IR_high_EI_poor', 'SP_given_IR_high_EI_average', 'SP_given_IR_high_EI_good'\n",
        "    ])\n",
        "    sp_df['SP_State'] = ['decrease', 'stable', 'increase']\n",
        "\n",
        "    # Combine all data into a single DataFrame\n",
        "    combined_df = pd.concat([ir_df, ei_df, sp_df], axis=1)\n",
        "\n",
        "    # Save the combined DataFrame as a single CSV file\n",
        "    combined_df.to_csv(filename, index=False)\n",
        "\n",
        "# Save outcomes in a CSV file\n",
        "def save_outcomes(data_dense, filename):\n",
        "    data_dense['IR'] = data_dense['IR'].map(ir_map)\n",
        "    data_dense['EI'] = data_dense['EI'].map(ei_map)\n",
        "    data_dense['SP'] = data_dense['SP'].map(sp_map)\n",
        "    data_dense.to_csv(filename, index=False)\n",
        "\n",
        "# Generate datasets for different sample sizes for the dense model\n",
        "sample_sizes = range(500, 10500, 500)\n",
        "for size in sample_sizes:\n",
        "    # Generate the CPDs\n",
        "    ir_probs, ei_given_ir_probs, sp_probs_reshaped = generate_cpds()\n",
        "\n",
        "    # Define CPDs for the dense model\n",
        "    cpd_ir = TabularCPD(variable='IR', variable_card=3, values=[[ir_probs[0]], [ir_probs[1]], [ir_probs[2]]])\n",
        "    cpd_ei_dense = TabularCPD(variable='EI', variable_card=3,\n",
        "                              values=ei_given_ir_probs,\n",
        "                              evidence=['IR'], evidence_card=[3])\n",
        "    cpd_sp_dense = TabularCPD(variable='SP', variable_card=3,\n",
        "                              values=sp_probs_reshaped,\n",
        "                              evidence=['IR', 'EI'], evidence_card=[3, 3])\n",
        "\n",
        "    dense_model.add_cpds(cpd_ir, cpd_ei_dense, cpd_sp_dense)\n",
        "\n",
        "    # Check if the model is valid\n",
        "    assert dense_model.check_model()\n",
        "\n",
        "    # Generate samples\n",
        "    sampler_dense = BayesianModelSampling(dense_model)\n",
        "    data_dense = sampler_dense.forward_sample(size=size)\n",
        "\n",
        "    # Save probabilities in one file\n",
        "    save_probabilities(ir_probs, ei_given_ir_probs, sp_probs_reshaped, f'probabilities_dense_{size}.csv')\n",
        "\n",
        "    # Save outcomes (low, medium, high) in another file\n",
        "    save_outcomes(data_dense, f'outcomes_dense_{size}.csv')\n",
        "\n",
        "# Notify the user that the process is done\n",
        "print(\"Data generation and saving complete for the dense model!\")"
      ],
      "metadata": {
        "id": "z2IBBn3I62tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Network Data Generation 500, 1000, 1500, ..., 10000 Samples (sparse)"
      ],
      "metadata": {
        "id": "MIagn-sFAjEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#np.random.seed(187)\n",
        "\n",
        "# Define the mappings for IR, EI, SP\n",
        "ir_map = {0: 'low', 1: 'medium', 2: 'high'}\n",
        "ei_map = {0: 'poor', 1: 'average', 2: 'good'}\n",
        "sp_map = {0: 'decrease', 1: 'stable', 2: 'increase'}\n",
        "\n",
        "# Define the sparse Bayesian Network\n",
        "sparse_model = BayesianNetwork([('IR', 'SP'), ('EI', 'SP')])\n",
        "\n",
        "# Function to generate CPDs for the sparse model\n",
        "def generate_cpds_sparse():\n",
        "    # Generate probabilities for IR (unconditional)\n",
        "    ir_probs = np.random.rand(3)\n",
        "    ir_probs /= ir_probs.sum()  # Normalize to make it a valid probability distribution\n",
        "\n",
        "    # Generate unconditional probabilities for EI (no dependency on IR)\n",
        "    ei_probs = np.random.rand(3)\n",
        "    ei_probs /= ei_probs.sum()\n",
        "\n",
        "    # Generate conditional probabilities for SP given IR and EI\n",
        "    sp_probs = np.random.rand(3, 3, 3)\n",
        "    sp_probs /= sp_probs.sum(axis=0, keepdims=True)\n",
        "\n",
        "    sp_probs_reshaped = sp_probs.reshape(3, -1)\n",
        "\n",
        "    return ir_probs, ei_probs, sp_probs_reshaped\n",
        "\n",
        "# Save probabilities in a single CSV file\n",
        "def save_probabilities_sparse(ir_probs, ei_probs, sp_probs, filename):\n",
        "    # Create a DataFrame for IR probabilities\n",
        "    ir_df = pd.DataFrame({\n",
        "        'IR_State': ['low', 'medium', 'high'],\n",
        "        'IR_Prob': ir_probs\n",
        "    })\n",
        "\n",
        "    # Create a DataFrame for EI probabilities (since it's not conditional on IR)\n",
        "    ei_df = pd.DataFrame({\n",
        "        'EI_State': ['poor', 'average', 'good'],\n",
        "        'EI_Prob': ei_probs\n",
        "    })\n",
        "\n",
        "    # Create a DataFrame for SP given IR and EI probabilities\n",
        "    sp_df = pd.DataFrame(sp_probs, columns=[\n",
        "        'SP_given_IR_low_EI_poor', 'SP_given_IR_low_EI_average', 'SP_given_IR_low_EI_good',\n",
        "        'SP_given_IR_medium_EI_poor', 'SP_given_IR_medium_EI_average', 'SP_given_IR_medium_EI_good',\n",
        "        'SP_given_IR_high_EI_poor', 'SP_given_IR_high_EI_average', 'SP_given_IR_high_EI_good'\n",
        "    ])\n",
        "    sp_df['SP_State'] = ['decrease', 'stable', 'increase']\n",
        "\n",
        "    # Combine all data into a single DataFrame\n",
        "    combined_df = pd.concat([ir_df, ei_df, sp_df], axis=1)\n",
        "\n",
        "    # Save the combined DataFrame as a single CSV file\n",
        "    combined_df.to_csv(filename, index=False)\n",
        "\n",
        "# Save outcomes in a CSV file\n",
        "def save_outcomes_sparse(data_sparse, filename):\n",
        "    data_sparse['IR'] = data_sparse['IR'].map(ir_map)\n",
        "    data_sparse['EI'] = data_sparse['EI'].map(ei_map)\n",
        "    data_sparse['SP'] = data_sparse['SP'].map(sp_map)\n",
        "    data_sparse.to_csv(filename, index=False)\n",
        "\n",
        "# Generate datasets for different sample sizes for the sparse model\n",
        "sample_sizes = range(500, 10500, 500)\n",
        "for size in sample_sizes:\n",
        "    # Generate the CPDs\n",
        "    ir_probs, ei_probs, sp_probs_reshaped = generate_cpds_sparse()\n",
        "\n",
        "    # Define CPDs for the sparse model\n",
        "    cpd_ir = TabularCPD(variable='IR', variable_card=3, values=[[ir_probs[0]], [ir_probs[1]], [ir_probs[2]]])\n",
        "    cpd_ei_sparse = TabularCPD(variable='EI', variable_card=3, values=[[ei_probs[0]], [ei_probs[1]], [ei_probs[2]]])\n",
        "    cpd_sp_sparse = TabularCPD(variable='SP', variable_card=3,\n",
        "                               values=sp_probs_reshaped,\n",
        "                               evidence=['IR', 'EI'], evidence_card=[3, 3])\n",
        "\n",
        "    sparse_model.add_cpds(cpd_ir, cpd_ei_sparse, cpd_sp_sparse)\n",
        "\n",
        "    # Check if the model is valid\n",
        "    assert sparse_model.check_model()\n",
        "\n",
        "    # Generate samples\n",
        "    sampler_sparse = BayesianModelSampling(sparse_model)\n",
        "    data_sparse = sampler_sparse.forward_sample(size=size)\n",
        "\n",
        "    # Save probabilities in one file\n",
        "    save_probabilities_sparse(ir_probs, ei_probs, sp_probs_reshaped, f'probabilities_sparse_{size}.csv')\n",
        "\n",
        "    # Save outcomes (low, medium, high) in another file\n",
        "    save_outcomes_sparse(data_sparse, f'outcomes_sparse_{size}.csv')\n",
        "\n",
        "# Notify the user that the process is done\n",
        "print(\"Data generation and saving complete for the sparse model!\")"
      ],
      "metadata": {
        "id": "I_DGt5k4AgnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "3UYRwm0BFPvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Control Experiment (dense)"
      ],
      "metadata": {
        "id": "961QnTOfDbQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BIC"
      ],
      "metadata": {
        "id": "qA6PrVXnW1yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sizes to loop through\n",
        "sample_sizes = range(500, 10500, 500)\n",
        "\n",
        "# Loop through each sample size\n",
        "for sample_size in sample_sizes:\n",
        "    print(f\"\\nProcessing sample size: {sample_size}\")\n",
        "\n",
        "    # Load the dense dataset for the current sample size\n",
        "    dense_data_file = f'outcomes_dense_{sample_size}.csv'\n",
        "    df_dense = pd.read_csv(dense_data_file)\n",
        "\n",
        "    # Manually encode categorical variables for IR, EI, and SP\n",
        "    ir_map = {'low': 0, 'medium': 1, 'high': 2}\n",
        "    ei_map = {'poor': 0, 'average': 1, 'good': 2}\n",
        "    sp_map = {'decrease': 0, 'stable': 1, 'increase': 2}\n",
        "\n",
        "    df_dense['IR_encoded'] = df_dense['IR'].map(ir_map)\n",
        "    df_dense['EI_encoded'] = df_dense['EI'].map(ei_map)\n",
        "    df_dense['SP_encoded'] = df_dense['SP'].map(sp_map)\n",
        "\n",
        "    # Define the Hill-Climb structure learning algorithm\n",
        "    hc = HillClimbSearch(df_dense[['IR_encoded', 'EI_encoded', 'SP_encoded']])\n",
        "    scoring_method = BicScore(df_dense[['IR_encoded', 'EI_encoded', 'SP_encoded']])\n",
        "\n",
        "    # Estimate the best structure\n",
        "    best_dag = hc.estimate(scoring_method=scoring_method)\n",
        "    best_model = BayesianNetwork(best_dag.edges())\n",
        "\n",
        "    # Check if all nodes are included in the learned structure\n",
        "    nodes_in_structure = set(best_model.nodes())\n",
        "    required_nodes = {'IR_encoded', 'EI_encoded', 'SP_encoded'}\n",
        "\n",
        "    if not required_nodes.issubset(nodes_in_structure):\n",
        "        print(\"\\nNot all nodes are connected. Adding a dummy variable.\")\n",
        "        # Add a dummy variable to the dataset\n",
        "        df_dense['dummy'] = 0\n",
        "\n",
        "        # Re-estimate the structure with the dummy variable\n",
        "        hc = HillClimbSearch(df_dense)\n",
        "        scoring_method = BicScore(df_dense)\n",
        "        best_dag = hc.estimate(scoring_method=scoring_method)\n",
        "        best_model = BayesianNetwork(best_dag.edges())\n",
        "\n",
        "    # Display the learned structure (edges of the Bayesian Network)\n",
        "    print(f\"\\nLearned Structure (Edges) for {sample_size} samples:\")\n",
        "    print(best_model.edges())\n",
        "\n",
        "    # Calculate and display the BIC score\n",
        "    bic_score = scoring_method.score(best_model)\n",
        "    print(f\"\\nBIC Score for {sample_size} samples: {bic_score:.4f}\")\n",
        "\n",
        "    # Learn the CPDs using Maximum Likelihood Estimation (MLE)\n",
        "    best_model.fit(df_dense, estimator=MaximumLikelihoodEstimator)\n",
        "\n",
        "    # Check if the model is valid after learning the parameters\n",
        "    assert best_model.check_model()\n",
        "\n",
        "    # Print the learned CPDs (Conditional Probability Distributions)\n",
        "    for cpd in best_model.get_cpds():\n",
        "        print(\"\\nCPD of\", cpd.variable)\n",
        "        print(cpd)\n",
        "\n",
        "    # Create an inference object for the best model\n",
        "    inference = VariableElimination(best_model)\n",
        "\n",
        "    # Placeholder to store predictions\n",
        "    predicted_sp_labels = []\n",
        "\n",
        "    # Loop through each row in the dense dataset to make predictions\n",
        "    for index, row in df_dense.iterrows():\n",
        "        # Prepare the evidence from the dataset (IR_encoded and EI_encoded)\n",
        "        sample_input = {'IR_encoded': int(row['IR_encoded']), 'EI_encoded': int(row['EI_encoded'])}\n",
        "\n",
        "        # Perform inference to predict the distribution for SP_encoded (Stock Price)\n",
        "        predicted_sp_distribution = inference.query(variables=['SP_encoded'], evidence=sample_input)\n",
        "\n",
        "        # Extract the most likely SP_encoded class\n",
        "        predicted_sp_class = predicted_sp_distribution.values.argmax()\n",
        "        sp_reverse_map = {0: 'decrease', 1: 'stable', 2: 'increase'}\n",
        "        predicted_sp_label = sp_reverse_map[predicted_sp_class]\n",
        "\n",
        "        # Store the predicted label\n",
        "        predicted_sp_labels.append(predicted_sp_label)\n",
        "\n",
        "    # Convert the list of predicted labels into a DataFrame for easier comparison\n",
        "    predicted_results_df = pd.DataFrame({\n",
        "        'IR': df_dense['IR'],  # Original IR column\n",
        "        'EI': df_dense['EI'],  # Original EI column\n",
        "        'Predicted_SP': predicted_sp_labels  # Predicted SP column\n",
        "    })\n",
        "\n",
        "    # Add the actual SP values for comparison\n",
        "    predicted_results_df['Actual_SP'] = df_dense['SP']\n",
        "\n",
        "    # Calculate accuracy of predictions\n",
        "    accuracy = accuracy_score(predicted_results_df['Actual_SP'], predicted_results_df['Predicted_SP'])\n",
        "    print(f\"\\nPrediction Accuracy for {sample_size} samples: {accuracy:.4f}\")\n",
        "\n",
        "    # Display the first few rows of predictions\n",
        "    print(f\"\\nPredicted Results for Dense Data (First 10 rows) for {sample_size} samples:\")\n",
        "    print(predicted_results_df.head(10))\n",
        "\n",
        "# Notify the user that the process is done\n",
        "print(\"\\nProcessing complete for all sample sizes.\")"
      ],
      "metadata": {
        "id": "hM2D1HHWVuMF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AIC"
      ],
      "metadata": {
        "id": "ahlVb62qW325"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sizes to loop through\n",
        "sample_sizes = range(500, 10500, 500)\n",
        "\n",
        "# Loop through each sample size\n",
        "for sample_size in sample_sizes:\n",
        "    print(f\"\\nProcessing sample size: {sample_size}\")\n",
        "\n",
        "    # Load the dense dataset for the current sample size\n",
        "    dense_data_file = f'outcomes_dense_{sample_size}.csv'\n",
        "    df_dense = pd.read_csv(dense_data_file)\n",
        "\n",
        "    # Manually encode categorical variables for IR, EI, and SP\n",
        "    ir_map = {'low': 0, 'medium': 1, 'high': 2}\n",
        "    ei_map = {'poor': 0, 'average': 1, 'good': 2}\n",
        "    sp_map = {'decrease': 0, 'stable': 1, 'increase': 2}\n",
        "\n",
        "    df_dense['IR_encoded'] = df_dense['IR'].map(ir_map)\n",
        "    df_dense['EI_encoded'] = df_dense['EI'].map(ei_map)\n",
        "    df_dense['SP_encoded'] = df_dense['SP'].map(sp_map)\n",
        "\n",
        "    # Define the Hill-Climb structure learning algorithm\n",
        "    hc = HillClimbSearch(df_dense[['IR_encoded', 'EI_encoded', 'SP_encoded']])\n",
        "    scoring_method = AICScore(df_dense[['IR_encoded', 'EI_encoded', 'SP_encoded']])  # Use AICScore instead of BicScore\n",
        "\n",
        "    # Estimate the best structure\n",
        "    best_dag = hc.estimate(scoring_method=scoring_method)\n",
        "    best_model = BayesianNetwork(best_dag.edges())\n",
        "\n",
        "    # Display the learned structure (edges of the Bayesian Network)\n",
        "    print(f\"\\nLearned Structure (Edges) for {sample_size} samples (AIC):\")\n",
        "    print(best_model.edges())\n",
        "\n",
        "    # Calculate and display the AIC score\n",
        "    aic_score = scoring_method.score(best_model)\n",
        "    print(f\"\\nAIC Score for {sample_size} samples: {aic_score:.4f}\")\n",
        "\n",
        "    # Learn the CPDs using Maximum Likelihood Estimation (MLE)\n",
        "    best_model.fit(df_dense[['IR_encoded', 'EI_encoded', 'SP_encoded']], estimator=MaximumLikelihoodEstimator)\n",
        "\n",
        "    # Check if the model is valid after learning the parameters\n",
        "    assert best_model.check_model()\n",
        "\n",
        "    # Print the learned CPDs (Conditional Probability Distributions)\n",
        "    for cpd in best_model.get_cpds():\n",
        "        print(\"\\nCPD of\", cpd.variable)\n",
        "        print(cpd)\n",
        "\n",
        "    # Create an inference object for the best model\n",
        "    inference = VariableElimination(best_model)\n",
        "\n",
        "    # Placeholder to store predictions\n",
        "    predicted_sp_labels = []\n",
        "\n",
        "    # Loop through each row in the dense dataset to make predictions\n",
        "    for index, row in df_dense.iterrows():\n",
        "        # Prepare the evidence from the dataset (IR_encoded and EI_encoded)\n",
        "        sample_input = {'IR_encoded': int(row['IR_encoded']), 'EI_encoded': int(row['EI_encoded'])}\n",
        "\n",
        "        # Perform inference to predict the distribution for SP_encoded (Stock Price)\n",
        "        predicted_sp_distribution = inference.query(variables=['SP_encoded'], evidence=sample_input)\n",
        "\n",
        "        # Extract the most likely SP_encoded class\n",
        "        predicted_sp_class = predicted_sp_distribution.values.argmax()\n",
        "        sp_reverse_map = {0: 'decrease', 1: 'stable', 2: 'increase'}\n",
        "        predicted_sp_label = sp_reverse_map[predicted_sp_class]\n",
        "\n",
        "        # Store the predicted label\n",
        "        predicted_sp_labels.append(predicted_sp_label)\n",
        "\n",
        "    # Convert the list of predicted labels into a DataFrame for easier comparison\n",
        "    predicted_results_df = pd.DataFrame({\n",
        "        'IR': df_dense['IR'],  # Original IR column\n",
        "        'EI': df_dense['EI'],  # Original EI column\n",
        "        'Predicted_SP': predicted_sp_labels  # Predicted SP column\n",
        "    })\n",
        "\n",
        "    # Add the actual SP values for comparison\n",
        "    predicted_results_df['Actual_SP'] = df_dense['SP']\n",
        "\n",
        "    # Calculate accuracy of predictions\n",
        "    accuracy = accuracy_score(predicted_results_df['Actual_SP'], predicted_results_df['Predicted_SP'])\n",
        "    print(f\"\\nPrediction Accuracy for {sample_size} samples (AIC): {accuracy:.4f}\")\n",
        "\n",
        "    # Display the first few rows of predictions\n",
        "    print(f\"\\nPredicted Results for Dense Data (First 10 rows) for {sample_size} samples (AIC):\")\n",
        "    print(predicted_results_df.head(10))\n",
        "\n",
        "    # Save the results if needed\n",
        "    results_filename = f'predicted_results_aic_{sample_size}.csv'\n",
        "    predicted_results_df.to_csv(results_filename, index=False)\n",
        "    print(f\"\\nResults saved to {results_filename}\")\n",
        "\n",
        "# Notify the user that the process is done\n",
        "print(\"\\nProcessing complete for all sample sizes using AIC.\")"
      ],
      "metadata": {
        "id": "cD-Qad1Anisp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Control Experiment (sparse)"
      ],
      "metadata": {
        "id": "fsXV0U1JmmbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BIC"
      ],
      "metadata": {
        "id": "WC0ytNf9U8xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sizes to loop through\n",
        "sample_sizes = range(500, 10500, 500)\n",
        "\n",
        "# Loop through each sample size\n",
        "for sample_size in sample_sizes:\n",
        "    print(f\"\\nProcessing sample size: {sample_size} (Sparse Data with BIC)\")\n",
        "\n",
        "    # Load the sparse dataset for the current sample size\n",
        "    sparse_data_file = f'outcomes_sparse_{sample_size}.csv'\n",
        "    df_sparse = pd.read_csv(sparse_data_file)\n",
        "\n",
        "    # Manually encode categorical variables for IR, EI, and SP\n",
        "    ir_map = {'low': 0, 'medium': 1, 'high': 2}\n",
        "    ei_map = {'poor': 0, 'average': 1, 'good': 2}\n",
        "    sp_map = {'decrease': 0, 'stable': 1, 'increase': 2}\n",
        "\n",
        "    df_sparse['IR_encoded'] = df_sparse['IR'].map(ir_map)\n",
        "    df_sparse['EI_encoded'] = df_sparse['EI'].map(ei_map)\n",
        "    df_sparse['SP_encoded'] = df_sparse['SP'].map(sp_map)\n",
        "\n",
        "    # Define the Hill-Climb structure learning algorithm\n",
        "    hc_bic = HillClimbSearch(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded']])\n",
        "    scoring_method_bic = BicScore(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded']])\n",
        "\n",
        "    # Estimate the best structure using BIC\n",
        "    best_dag_bic = hc_bic.estimate(scoring_method=scoring_method_bic)\n",
        "\n",
        "    # Ensure all required nodes are present in the model, even if not connected\n",
        "    best_model_bic = BayesianNetwork()\n",
        "    best_model_bic.add_nodes_from(['IR_encoded', 'EI_encoded', 'SP_encoded'])  # Add all nodes\n",
        "    best_model_bic.add_edges_from(best_dag_bic.edges())  # Add edges from the learned structure\n",
        "\n",
        "    # Check if all nodes are included in the learned structure\n",
        "    nodes_in_structure_bic = set(best_model_bic.nodes())\n",
        "    required_nodes = {'IR_encoded', 'EI_encoded', 'SP_encoded'}\n",
        "\n",
        "    if not required_nodes.issubset(nodes_in_structure_bic):\n",
        "        print(\"\\nNot all nodes are connected. Adding a dummy variable and ensuring all required nodes are present.\")\n",
        "        # Add a dummy variable to the dataset\n",
        "        df_sparse['Dummy_Node'] = 1  # Constant dummy node\n",
        "\n",
        "        # Ensure all required nodes are in the model by adding edges with the dummy node\n",
        "        for node in required_nodes:\n",
        "            if node not in nodes_in_structure_bic:\n",
        "                best_model_bic.add_edge('Dummy_Node', node)\n",
        "\n",
        "        # Re-estimate the structure with the dummy variable\n",
        "        hc_bic = HillClimbSearch(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded', 'Dummy_Node']])\n",
        "        scoring_method_bic = BicScore(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded', 'Dummy_Node']])\n",
        "        best_dag_bic = hc_bic.estimate(scoring_method=scoring_method_bic)\n",
        "        best_model_bic = BayesianNetwork(best_dag_bic.edges())\n",
        "\n",
        "    # Display the learned structure (edges of the Bayesian Network)\n",
        "    print(f\"\\nLearned Structure (Edges) for {sample_size} samples (BIC):\")\n",
        "    print(best_model_bic.edges())\n",
        "\n",
        "    # Learn the CPDs using Maximum Likelihood Estimation (MLE)\n",
        "    best_model_bic.fit(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded']], estimator=MaximumLikelihoodEstimator)\n",
        "\n",
        "    # Check if the model is valid after learning the parameters\n",
        "    assert best_model_bic.check_model()\n",
        "\n",
        "    # Print the learned CPDs (Conditional Probability Distributions)\n",
        "    for cpd in best_model_bic.get_cpds():\n",
        "        print(\"\\nCPD of\", cpd.variable)\n",
        "        print(cpd)\n",
        "\n",
        "    # Create an inference object for the BIC model\n",
        "    inference_bic = VariableElimination(best_model_bic)\n",
        "\n",
        "    # Placeholder to store predictions\n",
        "    predicted_sp_labels_bic = []\n",
        "\n",
        "    # Loop through each row in the dataset to make predictions using BIC\n",
        "    for index, row in df_sparse.iterrows():\n",
        "        # Prepare the evidence from the dataset (IR_encoded and EI_encoded)\n",
        "        sample_input = {'IR_encoded': int(row['IR_encoded']), 'EI_encoded': int(row['EI_encoded'])}\n",
        "\n",
        "        # Perform inference to predict the distribution for SP_encoded (Stock Price)\n",
        "        predicted_sp_distribution_bic = inference_bic.query(variables=['SP_encoded'], evidence=sample_input)\n",
        "\n",
        "        # Extract the most likely SP_encoded class\n",
        "        predicted_sp_class_bic = predicted_sp_distribution_bic.values.argmax()\n",
        "        sp_reverse_map = {0: 'decrease', 1: 'stable', 2: 'increase'}\n",
        "        predicted_sp_label_bic = sp_reverse_map[predicted_sp_class_bic]\n",
        "\n",
        "        # Store the predicted label\n",
        "        predicted_sp_labels_bic.append(predicted_sp_label_bic)\n",
        "\n",
        "    # Convert the list of predicted labels into a DataFrame for easier comparison\n",
        "    predicted_results_df_bic = pd.DataFrame({\n",
        "        'IR': df_sparse['IR'],  # Original IR column\n",
        "        'EI': df_sparse['EI'],  # Original EI column\n",
        "        'Predicted_SP': predicted_sp_labels_bic  # Predicted SP column\n",
        "    })\n",
        "\n",
        "    # Add the actual SP values for comparison\n",
        "    predicted_results_df_bic['Actual_SP'] = df_sparse['SP']\n",
        "\n",
        "    # Calculate accuracy of predictions for BIC\n",
        "    accuracy_bic = accuracy_score(predicted_results_df_bic['Actual_SP'], predicted_results_df_bic['Predicted_SP'])\n",
        "    print(f\"\\nPrediction Accuracy for {sample_size} samples (BIC): {accuracy_bic:.4f}\")\n",
        "\n",
        "    # Display the first few rows of predictions for BIC\n",
        "    print(f\"\\nPredicted Results for Sparse Data (First 10 rows) for {sample_size} samples (BIC):\")\n",
        "    print(predicted_results_df_bic.head(10))\n",
        "\n",
        "    # Calculate the BIC score for the Bayesian Network model\n",
        "    bic_score_value = scoring_method_bic.score(best_model_bic)\n",
        "\n",
        "    # Print the BIC score\n",
        "    print(f\"\\nBIC Score for {sample_size} samples: {bic_score_value:.4f}\")\n",
        "\n",
        "# Notify the user that the process is done\n",
        "print(\"\\nProcessing complete for all sample sizes using BIC (Sparse Data).\")"
      ],
      "metadata": {
        "id": "Fvx7mfYt4wkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AIC"
      ],
      "metadata": {
        "id": "RZgL1HaJVErd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sizes to loop through\n",
        "sample_sizes = range(500, 10500, 500)\n",
        "\n",
        "# Loop through each sample size\n",
        "for sample_size in sample_sizes:\n",
        "    print(f\"\\nProcessing sample size: {sample_size} (Sparse Data with AIC)\")\n",
        "\n",
        "    # Load the sparse dataset for the current sample size\n",
        "    sparse_data_file = f'outcomes_sparse_{sample_size}.csv'\n",
        "    df_sparse = pd.read_csv(sparse_data_file)\n",
        "\n",
        "    # Manually encode categorical variables for IR, EI, and SP\n",
        "    ir_map = {'low': 0, 'medium': 1, 'high': 2}\n",
        "    ei_map = {'poor': 0, 'average': 1, 'good': 2}\n",
        "    sp_map = {'decrease': 0, 'stable': 1, 'increase': 2}\n",
        "\n",
        "    df_sparse['IR_encoded'] = df_sparse['IR'].map(ir_map)\n",
        "    df_sparse['EI_encoded'] = df_sparse['EI'].map(ei_map)\n",
        "    df_sparse['SP_encoded'] = df_sparse['SP'].map(sp_map)\n",
        "\n",
        "    # Define the Hill-Climb structure learning algorithm\n",
        "    hc_aic = HillClimbSearch(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded']])\n",
        "    scoring_method_aic = AICScore(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded']])\n",
        "\n",
        "    # Estimate the best structure using AIC\n",
        "    best_dag_aic = hc_aic.estimate(scoring_method=scoring_method_aic)\n",
        "\n",
        "    # Ensure all required nodes are present in the model, even if not connected\n",
        "    best_model_aic = BayesianNetwork()\n",
        "    best_model_aic.add_nodes_from(['IR_encoded', 'EI_encoded', 'SP_encoded'])  # Add all nodes\n",
        "    best_model_aic.add_edges_from(best_dag_aic.edges())  # Add edges from the learned structure\n",
        "\n",
        "    # Check if all nodes are included in the learned structure\n",
        "    nodes_in_structure_aic = set(best_model_aic.nodes())\n",
        "    required_nodes = {'IR_encoded', 'EI_encoded', 'SP_encoded'}\n",
        "\n",
        "    if not required_nodes.issubset(nodes_in_structure_aic):\n",
        "        print(\"\\nNot all nodes are connected. Adding a dummy variable and ensuring all required nodes are present.\")\n",
        "        # Add a dummy variable to the dataset\n",
        "        df_sparse['Dummy_Node'] = 1  # Constant dummy node\n",
        "\n",
        "        # Ensure all required nodes are in the model by adding edges with the dummy node\n",
        "        for node in required_nodes:\n",
        "            if node not in nodes_in_structure_aic:\n",
        "                best_model_aic.add_edge('Dummy_Node', node)\n",
        "\n",
        "        # Re-estimate the structure with the dummy variable\n",
        "        hc_aic = HillClimbSearch(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded', 'Dummy_Node']])\n",
        "        scoring_method_aic = AICScore(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded', 'Dummy_Node']])\n",
        "        best_dag_aic = hc_aic.estimate(scoring_method=scoring_method_aic)\n",
        "        best_model_aic = BayesianNetwork(best_dag_aic.edges())\n",
        "\n",
        "    # Display the learned structure (edges of the Bayesian Network)\n",
        "    print(f\"\\nLearned Structure (Edges) for {sample_size} samples (AIC):\")\n",
        "    print(best_model_aic.edges())\n",
        "\n",
        "    # Learn the CPDs using Maximum Likelihood Estimation (MLE)\n",
        "    best_model_aic.fit(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded']], estimator=MaximumLikelihoodEstimator)\n",
        "\n",
        "    # Check if the model is valid after learning the parameters\n",
        "    assert best_model_aic.check_model()\n",
        "\n",
        "    # Print the learned CPDs (Conditional Probability Distributions)\n",
        "    for cpd in best_model_aic.get_cpds():\n",
        "        print(\"\\nCPD of\", cpd.variable)\n",
        "        print(cpd)\n",
        "\n",
        "    # Create an inference object for the AIC model\n",
        "    inference_aic = VariableElimination(best_model_aic)\n",
        "\n",
        "    # Placeholder to store predictions\n",
        "    predicted_sp_labels_aic = []\n",
        "\n",
        "    # Loop through each row in the dataset to make predictions using AIC\n",
        "    for index, row in df_sparse.iterrows():\n",
        "        # Prepare the evidence from the dataset (IR_encoded and EI_encoded)\n",
        "        sample_input = {'IR_encoded': int(row['IR_encoded']), 'EI_encoded': int(row['EI_encoded'])}\n",
        "\n",
        "        # Perform inference to predict the distribution for SP_encoded (Stock Price)\n",
        "        predicted_sp_distribution_aic = inference_aic.query(variables=['SP_encoded'], evidence=sample_input)\n",
        "\n",
        "        # Extract the most likely SP_encoded class\n",
        "        predicted_sp_class_aic = predicted_sp_distribution_aic.values.argmax()\n",
        "        sp_reverse_map = {0: 'decrease', 1: 'stable', 2: 'increase'}\n",
        "        predicted_sp_label_aic = sp_reverse_map[predicted_sp_class_aic]\n",
        "\n",
        "        # Store the predicted label\n",
        "        predicted_sp_labels_aic.append(predicted_sp_label_aic)\n",
        "\n",
        "    # Convert the list of predicted labels into a DataFrame for easier comparison\n",
        "    predicted_results_df_aic = pd.DataFrame({\n",
        "        'IR': df_sparse['IR'],  # Original IR column\n",
        "        'EI': df_sparse['EI'],  # Original EI column\n",
        "        'Predicted_SP': predicted_sp_labels_aic  # Predicted SP column\n",
        "    })\n",
        "\n",
        "    # Add the actual SP values for comparison\n",
        "    predicted_results_df_aic['Actual_SP'] = df_sparse['SP']\n",
        "\n",
        "    # Calculate accuracy of predictions for AIC\n",
        "    accuracy_aic = accuracy_score(predicted_results_df_aic['Actual_SP'], predicted_results_df_aic['Predicted_SP'])\n",
        "    print(f\"\\nPrediction Accuracy for {sample_size} samples (AIC): {accuracy_aic:.4f}\")\n",
        "\n",
        "    # Display the first few rows of predictions for AIC\n",
        "    print(f\"\\nPredicted Results for Sparse Data (First 10 rows) for {sample_size} samples (AIC):\")\n",
        "    print(predicted_results_df_aic.head(10))\n",
        "\n",
        "    # Calculate the AIC score for the Bayesian Network model\n",
        "    aic_score_value = scoring_method_aic.score(best_model_aic)\n",
        "\n",
        "    # Print the AIC score\n",
        "    print(f\"\\nAIC Score for {sample_size} samples: {aic_score_value:.4f}\")\n",
        "\n",
        "# Notify the user that the process is done\n",
        "print(\"\\nProcessing complete for all sample sizes using AIC (Sparse Data).\")"
      ],
      "metadata": {
        "id": "nT7o-asz4-Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "WBA6oWYhDrvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-L Divergence LBN Dense Data"
      ],
      "metadata": {
        "id": "0PV_oUD6r4ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BIC"
      ],
      "metadata": {
        "id": "NOARlE0FonQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to save K-L divergence to a file\n",
        "def save_kl_divergence(sample_size, kl_div_value, clear_file=False):\n",
        "    file_name = 'kl_div_LBN_dense_bic.csv'\n",
        "\n",
        "    # If it's the first run (clear_file is True), remove the file to start fresh\n",
        "    if clear_file and os.path.exists(file_name):\n",
        "        os.remove(file_name)\n",
        "\n",
        "    # Create a new file and write the header if it doesn't exist\n",
        "    if not os.path.exists(file_name):\n",
        "        with open(file_name, 'w') as f:\n",
        "            f.write('Size,LBN_Dense_BIC_Entropy\\n')  # Write the headers\n",
        "\n",
        "    # Append the K-L divergence for this sample size to the file\n",
        "    with open(file_name, 'a') as f:\n",
        "        f.write(f\"{sample_size},{kl_div_value:.4f}\\n\")\n",
        "\n",
        "# Sample sizes to loop through\n",
        "sample_sizes = range(500, 10500, 500)\n",
        "\n",
        "# Start fresh and clear the file on the first run\n",
        "clear_file = True\n",
        "\n",
        "# Loop through each sample size\n",
        "for sample_size in sample_sizes:\n",
        "    # Load the dense dataset and ground truth probabilities for the current sample size\n",
        "    dense_data_file = f'outcomes_dense_{sample_size}.csv'\n",
        "    ground_truth_probs_file = f'probabilities_dense_{sample_size}.csv'\n",
        "    df_dense = pd.read_csv(dense_data_file)\n",
        "    df_gt_probs = pd.read_csv(ground_truth_probs_file)\n",
        "\n",
        "    # Manually encode categorical variables for IR, EI, and SP\n",
        "    ir_map = {'low': 0, 'medium': 1, 'high': 2}\n",
        "    ei_map = {'poor': 0, 'average': 1, 'good': 2}\n",
        "    sp_map = {'decrease': 0, 'stable': 1, 'increase': 2}\n",
        "\n",
        "    df_dense['IR_encoded'] = df_dense['IR'].map(ir_map)\n",
        "    df_dense['EI_encoded'] = df_dense['EI'].map(ei_map)\n",
        "    df_dense['SP_encoded'] = df_dense['SP'].map(sp_map)\n",
        "\n",
        "    # Define the Hill-Climb structure learning algorithm\n",
        "    hc = HillClimbSearch(df_dense[['IR_encoded', 'EI_encoded', 'SP_encoded']])\n",
        "    scoring_method = BicScore(df_dense[['IR_encoded', 'EI_encoded', 'SP_encoded']])\n",
        "\n",
        "    # Estimate the best structure\n",
        "    best_dag = hc.estimate(scoring_method=scoring_method)\n",
        "\n",
        "    # Ensure SP_encoded is part of the model, even if not in best_dag\n",
        "    best_model = BayesianModel(best_dag.edges())\n",
        "    best_model.add_node('SP_encoded')\n",
        "\n",
        "    # Learn the CPDs using Maximum Likelihood Estimation (MLE)\n",
        "    best_model.fit(df_dense[['IR_encoded', 'EI_encoded', 'SP_encoded']], estimator=MaximumLikelihoodEstimator)\n",
        "\n",
        "    # Create an inference object for the best model\n",
        "    inference = VariableElimination(best_model)\n",
        "\n",
        "    # Placeholder to store K-L divergence values\n",
        "    kl_divergences = []\n",
        "\n",
        "    # Loop through each row in the dense dataset to make predictions\n",
        "    for index, row in df_dense.iterrows():\n",
        "        sample_input = {'IR_encoded': int(row['IR_encoded']), 'EI_encoded': int(row['EI_encoded'])}\n",
        "        predicted_sp_distribution = inference.query(variables=['SP_encoded'], evidence=sample_input)\n",
        "        predicted_probs = predicted_sp_distribution.values\n",
        "\n",
        "        ir_value = row['IR']\n",
        "        ei_value = row['EI']\n",
        "\n",
        "        col_prefix = f'SP_given_IR_{ir_value}_EI_{ei_value}'\n",
        "        ground_truth_probs = df_gt_probs.filter(like=col_prefix).values.flatten()\n",
        "\n",
        "        # Ensure the probabilities are non-zero to avoid division by zero\n",
        "        epsilon = 1e-10\n",
        "        ground_truth_probs = np.clip(ground_truth_probs, epsilon, 1)\n",
        "\n",
        "        # Compute K-L divergence (Learned BN vs Ground Truth)\n",
        "        kl_div = entropy(predicted_probs, ground_truth_probs)\n",
        "        kl_divergences.append(kl_div)\n",
        "\n",
        "    # Calculate the average K-L divergence over all samples\n",
        "    average_kl_divergence = np.mean(kl_divergences)\n",
        "\n",
        "    # Save the K-L divergence value to the CSV file\n",
        "    save_kl_divergence(sample_size, average_kl_divergence, clear_file=clear_file)\n",
        "\n",
        "    # Set clear_file to False after the first iteration\n",
        "    clear_file = False\n",
        "\n",
        "    # Print confirmation\n",
        "    print(f\"Average K-L Divergence for {sample_size} samples (BIC): {average_kl_divergence:.4f}\")\n",
        "\n",
        "# Notify the user that the process is done\n",
        "print(\"\\nK-L divergence calculations complete and saved to 'kl_div_LBN_dense_bic.csv'.\")"
      ],
      "metadata": {
        "id": "t_wpIHleDxrX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AIC"
      ],
      "metadata": {
        "id": "n8Q5fM3DmjcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to save K-L divergence to a file\n",
        "def save_kl_divergence(sample_size, kl_div_value, clear_file=False):\n",
        "    file_name = 'kl_div_LBN_dense_aic.csv'  # Changed filename to indicate AIC\n",
        "\n",
        "    # If it's the first run (clear_file is True), remove the file to start fresh\n",
        "    if clear_file and os.path.exists(file_name):\n",
        "        os.remove(file_name)\n",
        "\n",
        "    # Create a new file and write the header if it doesn't exist\n",
        "    if not os.path.exists(file_name):\n",
        "        with open(file_name, 'w') as f:\n",
        "            f.write('Size,LBN_Dense_AIC_Entropy\\n')  # Write the headers\n",
        "\n",
        "    # Append the K-L divergence for this sample size to the file\n",
        "    with open(file_name, 'a') as f:\n",
        "        f.write(f\"{sample_size},{kl_div_value:.4f}\\n\")\n",
        "\n",
        "# Sample sizes to loop through\n",
        "sample_sizes = range(500, 10500, 500)\n",
        "\n",
        "# Start fresh and clear the file on the first run\n",
        "clear_file = True\n",
        "\n",
        "# Loop through each sample size\n",
        "for sample_size in sample_sizes:\n",
        "    # Load the dense dataset and ground truth probabilities for the current sample size\n",
        "    dense_data_file = f'outcomes_dense_{sample_size}.csv'\n",
        "    ground_truth_probs_file = f'probabilities_dense_{sample_size}.csv'\n",
        "    df_dense = pd.read_csv(dense_data_file)\n",
        "    df_gt_probs = pd.read_csv(ground_truth_probs_file)\n",
        "\n",
        "    # Manually encode categorical variables for IR, EI, and SP\n",
        "    ir_map = {'low': 0, 'medium': 1, 'high': 2}\n",
        "    ei_map = {'poor': 0, 'average': 1, 'good': 2}\n",
        "    sp_map = {'decrease': 0, 'stable': 1, 'increase': 2}\n",
        "\n",
        "    df_dense['IR_encoded'] = df_dense['IR'].map(ir_map)\n",
        "    df_dense['EI_encoded'] = df_dense['EI'].map(ei_map)\n",
        "    df_dense['SP_encoded'] = df_dense['SP'].map(sp_map)\n",
        "\n",
        "    # Define the Hill-Climb structure learning algorithm\n",
        "    hc = HillClimbSearch(df_dense[['IR_encoded', 'EI_encoded', 'SP_encoded']])\n",
        "    scoring_method = AICScore(df_dense[['IR_encoded', 'EI_encoded', 'SP_encoded']])  # Use AICScore instead of BicScore\n",
        "\n",
        "    # Estimate the best structure\n",
        "    best_dag = hc.estimate(scoring_method=scoring_method)\n",
        "\n",
        "    # Ensure SP_encoded is part of the model, even if not in best_dag\n",
        "    best_model = BayesianModel(best_dag.edges())\n",
        "    best_model.add_node('SP_encoded')\n",
        "\n",
        "    # Learn the CPDs using Maximum Likelihood Estimation (MLE)\n",
        "    best_model.fit(df_dense[['IR_encoded', 'EI_encoded', 'SP_encoded']], estimator=MaximumLikelihoodEstimator)\n",
        "\n",
        "    # Create an inference object for the best model\n",
        "    inference = VariableElimination(best_model)\n",
        "\n",
        "    # Placeholder to store K-L divergence values\n",
        "    kl_divergences = []\n",
        "\n",
        "    # Loop through each row in the dense dataset to make predictions\n",
        "    for index, row in df_dense.iterrows():\n",
        "        sample_input = {'IR_encoded': int(row['IR_encoded']), 'EI_encoded': int(row['EI_encoded'])}\n",
        "        predicted_sp_distribution = inference.query(variables=['SP_encoded'], evidence=sample_input)\n",
        "        predicted_probs = predicted_sp_distribution.values\n",
        "\n",
        "        ir_value = row['IR']\n",
        "        ei_value = row['EI']\n",
        "\n",
        "        col_prefix = f'SP_given_IR_{ir_value}_EI_{ei_value}'\n",
        "        ground_truth_probs = df_gt_probs.filter(like=col_prefix).values.flatten()\n",
        "\n",
        "        # Ensure the probabilities are non-zero to avoid division by zero\n",
        "        epsilon = 1e-10\n",
        "        ground_truth_probs = np.clip(ground_truth_probs, epsilon, 1)\n",
        "\n",
        "        # Compute K-L divergence (Learned BN vs Ground Truth)\n",
        "        kl_div = entropy(predicted_probs, ground_truth_probs)\n",
        "        kl_divergences.append(kl_div)\n",
        "\n",
        "    # Calculate the average K-L divergence over all samples\n",
        "    average_kl_divergence = np.mean(kl_divergences)\n",
        "\n",
        "    # Save the K-L divergence value to the CSV file\n",
        "    save_kl_divergence(sample_size, average_kl_divergence, clear_file=clear_file)\n",
        "\n",
        "    # Set clear_file to False after the first iteration\n",
        "    clear_file = False\n",
        "\n",
        "    # Print confirmation\n",
        "    print(f\"Average K-L Divergence for {sample_size} samples (AIC): {average_kl_divergence:.4f}\")\n",
        "\n",
        "# Notify the user that the process is done\n",
        "print(\"\\nK-L divergence calculations complete and saved to 'kl_div_LBN_dense_aic.csv'.\")"
      ],
      "metadata": {
        "id": "j0v-9L1aoy_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-L Divergence LBN Sparse Data"
      ],
      "metadata": {
        "id": "zAZIoMNIrmqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BIC"
      ],
      "metadata": {
        "id": "dOIftie8ruSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to save K-L divergence to a file\n",
        "def save_kl_divergence(sample_size, kl_div_value, file_name, clear_file=False):\n",
        "    # If it's the first run (clear_file is True), remove the file to start fresh\n",
        "    if clear_file and os.path.exists(file_name):\n",
        "        os.remove(file_name)\n",
        "\n",
        "    # Create a new file and write the header if it doesn't exist\n",
        "    if not os.path.exists(file_name):\n",
        "        with open(file_name, 'w') as f:\n",
        "            f.write('Size,LBN_Sparse_BIC_Entropy\\n')  # Write the headers\n",
        "\n",
        "    # Append the K-L divergence for this sample size to the file\n",
        "    with open(file_name, 'a') as f:\n",
        "        f.write(f\"{sample_size},{kl_div_value:.4f}\\n\")\n",
        "\n",
        "# Sample sizes to loop through\n",
        "sample_sizes = range(100, 1100, 100)\n",
        "\n",
        "# File name for BIC K-L divergence results\n",
        "kl_div_bic_file = 'kl_div_LBN_sparse_bic.csv'\n",
        "\n",
        "# Start fresh and clear the file on the first run\n",
        "clear_file = True\n",
        "\n",
        "# Loop through each sample size for BIC\n",
        "for sample_size in sample_sizes:\n",
        "    print(f\"\\nCalculating K-L Divergence for BIC with sample size: {sample_size}\")\n",
        "\n",
        "    # Load the sparse dataset and ground truth probabilities for the current sample size\n",
        "    sparse_data_file = f'outcomes_sparse_{sample_size}.csv'\n",
        "    ground_truth_probs_file = f'probabilities_sparse_{sample_size}.csv'\n",
        "    df_sparse = pd.read_csv(sparse_data_file)\n",
        "    df_gt_probs = pd.read_csv(ground_truth_probs_file)\n",
        "\n",
        "    # Manually encode categorical variables for IR, EI, and SP\n",
        "    ir_map = {'low': 0, 'medium': 1, 'high': 2}\n",
        "    ei_map = {'poor': 0, 'average': 1, 'good': 2}\n",
        "    sp_map = {'decrease': 0, 'stable': 1, 'increase': 2}\n",
        "\n",
        "    df_sparse['IR_encoded'] = df_sparse['IR'].map(ir_map)\n",
        "    df_sparse['EI_encoded'] = df_sparse['EI'].map(ei_map)\n",
        "    df_sparse['SP_encoded'] = df_sparse['SP'].map(sp_map)\n",
        "\n",
        "    # Define the Hill-Climb structure learning algorithm\n",
        "    hc = HillClimbSearch(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded']])\n",
        "    scoring_method = BicScore(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded']])\n",
        "\n",
        "    # Estimate the best structure\n",
        "    best_dag = hc.estimate(scoring_method=scoring_method)\n",
        "\n",
        "    # Ensure all required nodes are present in the model, even if not connected\n",
        "    best_model = BayesianNetwork()\n",
        "    best_model.add_nodes_from(['IR_encoded', 'EI_encoded', 'SP_encoded'])  # Add all nodes\n",
        "    best_model.add_edges_from(best_dag.edges())  # Add edges from the learned structure\n",
        "\n",
        "    # Add a dummy variable if required nodes are not connected\n",
        "    nodes_in_structure = set(best_model.nodes())\n",
        "    required_nodes = {'IR_encoded', 'EI_encoded', 'SP_encoded'}\n",
        "    if not required_nodes.issubset(nodes_in_structure):\n",
        "        df_sparse['Dummy_Node'] = 1  # Constant dummy node\n",
        "        hc = HillClimbSearch(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded', 'Dummy_Node']])\n",
        "        scoring_method = BicScore(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded', 'Dummy_Node']])\n",
        "        best_dag = hc.estimate(scoring_method=scoring_method)\n",
        "        best_model = BayesianNetwork(best_dag.edges())\n",
        "\n",
        "    # Learn the CPDs using Maximum Likelihood Estimation (MLE)\n",
        "    best_model.fit(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded']], estimator=MaximumLikelihoodEstimator)\n",
        "\n",
        "    # Create an inference object for the best model\n",
        "    inference = VariableElimination(best_model)\n",
        "\n",
        "    # Placeholder to store K-L divergence values\n",
        "    kl_divergences = []\n",
        "\n",
        "    # Loop through each row in the sparse dataset to make predictions\n",
        "    for index, row in df_sparse.iterrows():\n",
        "        sample_input = {'IR_encoded': int(row['IR_encoded']), 'EI_encoded': int(row['EI_encoded'])}\n",
        "        predicted_sp_distribution = inference.query(variables=['SP_encoded'], evidence=sample_input)\n",
        "        predicted_probs = predicted_sp_distribution.values\n",
        "\n",
        "        ir_value = row['IR']\n",
        "        ei_value = row['EI']\n",
        "\n",
        "        col_prefix = f'SP_given_IR_{ir_value}_EI_{ei_value}'\n",
        "        ground_truth_probs = df_gt_probs.filter(like=col_prefix).values.flatten()\n",
        "\n",
        "        # Ensure the probabilities are non-zero to avoid division by zero\n",
        "        epsilon = 1e-10\n",
        "        ground_truth_probs = np.clip(ground_truth_probs, epsilon, 1)\n",
        "\n",
        "        # Compute K-L divergence (Learned BN vs Ground Truth)\n",
        "        kl_div = entropy(predicted_probs, ground_truth_probs)\n",
        "        kl_divergences.append(kl_div)\n",
        "\n",
        "    # Calculate the average K-L divergence over all samples\n",
        "    average_kl_divergence = np.mean(kl_divergences)\n",
        "\n",
        "    # Save the K-L divergence value to the CSV file\n",
        "    save_kl_divergence(sample_size, average_kl_divergence, kl_div_bic_file, clear_file=clear_file)\n",
        "\n",
        "    # Set clear_file to False after the first iteration\n",
        "    clear_file = False\n",
        "\n",
        "    # Print confirmation\n",
        "    print(f\"Average K-L Divergence for {sample_size} samples (BIC): {average_kl_divergence:.4f}\")\n",
        "\n",
        "# Notify the user that the process is done\n",
        "print(\"\\nK-L divergence calculations for BIC complete and saved to 'kl_div_LBN_sparse_BIC.csv'.\")"
      ],
      "metadata": {
        "id": "gq6lf-dIry6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AIC"
      ],
      "metadata": {
        "id": "kPruDfsurvzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to save K-L divergence to a file\n",
        "def save_kl_divergence(sample_size, kl_div_value, file_name, clear_file=False):\n",
        "    # If it's the first run (clear_file is True), remove the file to start fresh\n",
        "    if clear_file and os.path.exists(file_name):\n",
        "        os.remove(file_name)\n",
        "\n",
        "    # Create a new file and write the header if it doesn't exist\n",
        "    if not os.path.exists(file_name):\n",
        "        with open(file_name, 'w') as f:\n",
        "            f.write('Size,LBN_Sparse_AIC_Entropy\\n')  # Write the headers\n",
        "\n",
        "    # Append the K-L divergence for this sample size to the file\n",
        "    with open(file_name, 'a') as f:\n",
        "        f.write(f\"{sample_size},{kl_div_value:.4f}\\n\")\n",
        "\n",
        "# Sample sizes to loop through\n",
        "sample_sizes = range(100, 1100, 100)\n",
        "\n",
        "# File name for AIC K-L divergence results\n",
        "kl_div_aic_file = 'kl_div_LBN_sparse_aic.csv'\n",
        "\n",
        "# Start fresh and clear the file on the first run\n",
        "clear_file = True\n",
        "\n",
        "# Loop through each sample size for AIC\n",
        "for sample_size in sample_sizes:\n",
        "    print(f\"\\nCalculating K-L Divergence for AIC with sample size: {sample_size}\")\n",
        "\n",
        "    # Load the sparse dataset and ground truth probabilities for the current sample size\n",
        "    sparse_data_file = f'outcomes_sparse_{sample_size}.csv'\n",
        "    ground_truth_probs_file = f'probabilities_sparse_{sample_size}.csv'\n",
        "    df_sparse = pd.read_csv(sparse_data_file)\n",
        "    df_gt_probs = pd.read_csv(ground_truth_probs_file)\n",
        "\n",
        "    # Manually encode categorical variables for IR, EI, and SP\n",
        "    ir_map = {'low': 0, 'medium': 1, 'high': 2}\n",
        "    ei_map = {'poor': 0, 'average': 1, 'good': 2}\n",
        "    sp_map = {'decrease': 0, 'stable': 1, 'increase': 2}\n",
        "\n",
        "    df_sparse['IR_encoded'] = df_sparse['IR'].map(ir_map)\n",
        "    df_sparse['EI_encoded'] = df_sparse['EI'].map(ei_map)\n",
        "    df_sparse['SP_encoded'] = df_sparse['SP'].map(sp_map)\n",
        "\n",
        "    # Define the Hill-Climb structure learning algorithm\n",
        "    hc = HillClimbSearch(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded']])\n",
        "    scoring_method = AICScore(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded']])\n",
        "\n",
        "    # Estimate the best structure\n",
        "    best_dag = hc.estimate(scoring_method=scoring_method)\n",
        "\n",
        "    # Ensure all required nodes are present in the model, even if not connected\n",
        "    best_model = BayesianNetwork()\n",
        "    best_model.add_nodes_from(['IR_encoded', 'EI_encoded', 'SP_encoded'])  # Add all nodes\n",
        "    best_model.add_edges_from(best_dag.edges())  # Add edges from the learned structure\n",
        "\n",
        "    # Add a dummy variable if required nodes are not connected\n",
        "    nodes_in_structure = set(best_model.nodes())\n",
        "    required_nodes = {'IR_encoded', 'EI_encoded', 'SP_encoded'}\n",
        "    if not required_nodes.issubset(nodes_in_structure):\n",
        "        df_sparse['Dummy_Node'] = 1  # Constant dummy node\n",
        "        hc = HillClimbSearch(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded', 'Dummy_Node']])\n",
        "        scoring_method = AICScore(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded', 'Dummy_Node']])\n",
        "        best_dag = hc.estimate(scoring_method=scoring_method)\n",
        "        best_model = BayesianNetwork(best_dag.edges())\n",
        "\n",
        "    # Learn the CPDs using Maximum Likelihood Estimation (MLE)\n",
        "    best_model.fit(df_sparse[['IR_encoded', 'EI_encoded', 'SP_encoded']], estimator=MaximumLikelihoodEstimator)\n",
        "\n",
        "    # Create an inference object for the best model\n",
        "    inference = VariableElimination(best_model)\n",
        "\n",
        "    # Placeholder to store K-L divergence values\n",
        "    kl_divergences = []\n",
        "\n",
        "    # Loop through each row in the sparse dataset to make predictions\n",
        "    for index, row in df_sparse.iterrows():\n",
        "        sample_input = {'IR_encoded': int(row['IR_encoded']), 'EI_encoded': int(row['EI_encoded'])}\n",
        "        predicted_sp_distribution = inference.query(variables=['SP_encoded'], evidence=sample_input)\n",
        "        predicted_probs = predicted_sp_distribution.values\n",
        "\n",
        "        ir_value = row['IR']\n",
        "        ei_value = row['EI']\n",
        "\n",
        "        col_prefix = f'SP_given_IR_{ir_value}_EI_{ei_value}'\n",
        "        ground_truth_probs = df_gt_probs.filter(like=col_prefix).values.flatten()\n",
        "\n",
        "        # Ensure the probabilities are non-zero to avoid division by zero\n",
        "        epsilon = 1e-10\n",
        "        ground_truth_probs = np.clip(ground_truth_probs, epsilon, 1)\n",
        "\n",
        "        # Compute K-L divergence (Learned BN vs Ground Truth)\n",
        "        kl_div = entropy(predicted_probs, ground_truth_probs)\n",
        "        kl_divergences.append(kl_div)\n",
        "\n",
        "    # Calculate the average K-L divergence over all samples\n",
        "    average_kl_divergence = np.mean(kl_divergences)\n",
        "\n",
        "    # Save the K-L divergence value to the CSV file\n",
        "    save_kl_divergence(sample_size, average_kl_divergence, kl_div_aic_file, clear_file=clear_file)\n",
        "\n",
        "    # Set clear_file to False after the first iteration\n",
        "    clear_file = False\n",
        "\n",
        "    # Print confirmation\n",
        "    print(f\"Average K-L Divergence for {sample_size} samples (AIC): {average_kl_divergence:.4f}\")\n",
        "\n",
        "# Notify the user that the process is done\n",
        "print(\"\\nK-L divergence calculations for AIC complete and saved to 'kl_div_LBN_sparse_AIC.csv'.\")"
      ],
      "metadata": {
        "id": "mKtdZbjg7oPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "F2CAVYjgE_5T"
      }
    }
  ]
}